{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99ca25dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247b4a64",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d0a37deb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning: invalid escape sequence '\\d'\n",
      "<>:1: SyntaxWarning: invalid escape sequence '\\d'\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_25816\\2879841595.py:1: SyntaxWarning: invalid escape sequence '\\d'\n",
      "  data = pd.read_csv('..\\datasets\\cardio_train.csv', sep=';')\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('..\\datasets\\cardio_train.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a021dc28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "      <th>ap_hi</th>\n",
       "      <th>ap_lo</th>\n",
       "      <th>cholesterol</th>\n",
       "      <th>gluc</th>\n",
       "      <th>smoke</th>\n",
       "      <th>alco</th>\n",
       "      <th>active</th>\n",
       "      <th>cardio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>18393</td>\n",
       "      <td>2</td>\n",
       "      <td>168</td>\n",
       "      <td>62.0</td>\n",
       "      <td>110</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>20228</td>\n",
       "      <td>1</td>\n",
       "      <td>156</td>\n",
       "      <td>85.0</td>\n",
       "      <td>140</td>\n",
       "      <td>90</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>18857</td>\n",
       "      <td>1</td>\n",
       "      <td>165</td>\n",
       "      <td>64.0</td>\n",
       "      <td>130</td>\n",
       "      <td>70</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>17623</td>\n",
       "      <td>2</td>\n",
       "      <td>169</td>\n",
       "      <td>82.0</td>\n",
       "      <td>150</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>17474</td>\n",
       "      <td>1</td>\n",
       "      <td>156</td>\n",
       "      <td>56.0</td>\n",
       "      <td>100</td>\n",
       "      <td>60</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69995</th>\n",
       "      <td>99993</td>\n",
       "      <td>19240</td>\n",
       "      <td>2</td>\n",
       "      <td>168</td>\n",
       "      <td>76.0</td>\n",
       "      <td>120</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69996</th>\n",
       "      <td>99995</td>\n",
       "      <td>22601</td>\n",
       "      <td>1</td>\n",
       "      <td>158</td>\n",
       "      <td>126.0</td>\n",
       "      <td>140</td>\n",
       "      <td>90</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69997</th>\n",
       "      <td>99996</td>\n",
       "      <td>19066</td>\n",
       "      <td>2</td>\n",
       "      <td>183</td>\n",
       "      <td>105.0</td>\n",
       "      <td>180</td>\n",
       "      <td>90</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69998</th>\n",
       "      <td>99998</td>\n",
       "      <td>22431</td>\n",
       "      <td>1</td>\n",
       "      <td>163</td>\n",
       "      <td>72.0</td>\n",
       "      <td>135</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69999</th>\n",
       "      <td>99999</td>\n",
       "      <td>20540</td>\n",
       "      <td>1</td>\n",
       "      <td>170</td>\n",
       "      <td>72.0</td>\n",
       "      <td>120</td>\n",
       "      <td>80</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>70000 rows Ã— 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id    age  gender  height  weight  ap_hi  ap_lo  cholesterol  gluc  \\\n",
       "0          0  18393       2     168    62.0    110     80            1     1   \n",
       "1          1  20228       1     156    85.0    140     90            3     1   \n",
       "2          2  18857       1     165    64.0    130     70            3     1   \n",
       "3          3  17623       2     169    82.0    150    100            1     1   \n",
       "4          4  17474       1     156    56.0    100     60            1     1   \n",
       "...      ...    ...     ...     ...     ...    ...    ...          ...   ...   \n",
       "69995  99993  19240       2     168    76.0    120     80            1     1   \n",
       "69996  99995  22601       1     158   126.0    140     90            2     2   \n",
       "69997  99996  19066       2     183   105.0    180     90            3     1   \n",
       "69998  99998  22431       1     163    72.0    135     80            1     2   \n",
       "69999  99999  20540       1     170    72.0    120     80            2     1   \n",
       "\n",
       "       smoke  alco  active  cardio  \n",
       "0          0     0       1       0  \n",
       "1          0     0       1       1  \n",
       "2          0     0       0       1  \n",
       "3          0     0       1       1  \n",
       "4          0     0       0       0  \n",
       "...      ...   ...     ...     ...  \n",
       "69995      1     0       1       0  \n",
       "69996      0     0       1       1  \n",
       "69997      0     1       0       1  \n",
       "69998      0     0       0       1  \n",
       "69999      0     0       1       0  \n",
       "\n",
       "[70000 rows x 13 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "879cc2a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'age', 'gender', 'height', 'weight', 'ap_hi', 'ap_lo',\n",
       "       'cholesterol', 'gluc', 'smoke', 'alco', 'active', 'cardio'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b287a872",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id             0\n",
       "age            0\n",
       "gender         0\n",
       "height         0\n",
       "weight         0\n",
       "ap_hi          0\n",
       "ap_lo          0\n",
       "cholesterol    0\n",
       "gluc           0\n",
       "smoke          0\n",
       "alco           0\n",
       "active         0\n",
       "cardio         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "09d462f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(data, scaler=None, fit_scaler=True):\n",
    "    df = data.copy()\n",
    "    \n",
    "    df['age_yr'] = df['age'] / 365.25\n",
    "\n",
    "    df['bmi'] = df['weight'] / (df['height']/100)**2\n",
    "\n",
    "    df['map'] = (2/3 * df['ap_lo']) + (1/3 * df['ap_hi'])\n",
    "\n",
    "    df['pp'] = df['ap_hi'] - df['ap_lo']\n",
    "\n",
    "    df['lifestyle'] = df['active'] - (df['smoke'] + df['alco'])\n",
    "\n",
    "    df['x_syndrome'] = (\n",
    "        (df['cholesterol'] > 1) &\n",
    "        (df['gluc'] > 1) &\n",
    "        ((df['ap_hi'] > 130) | (df['ap_lo'] > 85))\n",
    "    ).astype(int)\n",
    "\n",
    "    num_features = ['age_yr', 'height', 'weight', 'ap_hi', 'ap_lo', \n",
    "                    'bmi', 'pp', 'lifestyle', 'map']\n",
    "\n",
    "    if fit_scaler:\n",
    "        scaler = StandardScaler()\n",
    "        df[num_features] = scaler.fit_transform(df[num_features])\n",
    "    else:\n",
    "        df[num_features] = scaler.transform(df[num_features])\n",
    "\n",
    "    df = df.drop(columns=['id', 'age'])\n",
    "\n",
    "    return df, scaler\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86dbfc45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "y = data['cardio']\n",
    "X = data.drop('cardio', axis=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    " scaler = preprocessing(X_train, fit_scaler=True)\n",
    "X_test_prep, _ = preprocessing(X_test, scaler = scaler, fit_scaler=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ae9c22e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CVSDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = torch.tensor(data.values, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(labels.values, dtype=torch.long)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index], self.labels[index]\n",
    "    \n",
    "train_data = CVSDataset(X_train_prep, y_train)\n",
    "test_data = CVSDataset(X_test_prep, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9215fee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_data, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=16, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "31b18fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleANN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SimpleANN, self).__init__()\n",
    "\n",
    "        self.hidden = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.output = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.hidden(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.output(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    "    \n",
    "input_size = X_train_prep.shape[1]\n",
    "hidden_size = 8\n",
    "output_size = 2\n",
    "\n",
    "model = SimpleANN(input_size, hidden_size, output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "87381ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c97e9f04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0, Loss: 0.4864\n",
      "Epoch:1, Loss: 0.4623\n",
      "Epoch:2, Loss: 0.5731\n",
      "Epoch:3, Loss: 0.5895\n",
      "Epoch:4, Loss: 0.5353\n",
      "Epoch:5, Loss: 0.4700\n",
      "Epoch:6, Loss: 0.6479\n",
      "Epoch:7, Loss: 0.6662\n",
      "Epoch:8, Loss: 0.7251\n",
      "Epoch:9, Loss: 0.4928\n",
      "Epoch:10, Loss: 0.5547\n",
      "Epoch:11, Loss: 0.5723\n",
      "Epoch:12, Loss: 0.4729\n",
      "Epoch:13, Loss: 0.7011\n",
      "Epoch:14, Loss: 0.4248\n",
      "Epoch:15, Loss: 0.7052\n",
      "Epoch:16, Loss: 0.5684\n",
      "Epoch:17, Loss: 0.6155\n",
      "Epoch:18, Loss: 0.5072\n",
      "Epoch:19, Loss: 0.6735\n",
      "Epoch:20, Loss: 0.6758\n",
      "Epoch:21, Loss: 0.5964\n",
      "Epoch:22, Loss: 0.6243\n",
      "Epoch:23, Loss: 0.5638\n",
      "Epoch:24, Loss: 0.4777\n",
      "Epoch:25, Loss: 0.5670\n",
      "Epoch:26, Loss: 0.6397\n",
      "Epoch:27, Loss: 0.5598\n",
      "Epoch:28, Loss: 0.6699\n",
      "Epoch:29, Loss: 0.4997\n",
      "Epoch:30, Loss: 0.5151\n",
      "Epoch:31, Loss: 0.5772\n",
      "Epoch:32, Loss: 0.3742\n",
      "Epoch:33, Loss: 0.6631\n",
      "Epoch:34, Loss: 0.5207\n",
      "Epoch:35, Loss: 0.5698\n",
      "Epoch:36, Loss: 0.6362\n",
      "Epoch:37, Loss: 0.4658\n",
      "Epoch:38, Loss: 0.4630\n",
      "Epoch:39, Loss: 0.5290\n",
      "Epoch:40, Loss: 0.5805\n",
      "Epoch:41, Loss: 0.4431\n",
      "Epoch:42, Loss: 0.6334\n",
      "Epoch:43, Loss: 0.6683\n",
      "Epoch:44, Loss: 0.6222\n",
      "Epoch:45, Loss: 0.5100\n",
      "Epoch:46, Loss: 0.4220\n",
      "Epoch:47, Loss: 0.4720\n",
      "Epoch:48, Loss: 0.6500\n",
      "Epoch:49, Loss: 0.4629\n",
      "Epoch:50, Loss: 0.5491\n",
      "Epoch:51, Loss: 0.4550\n",
      "Epoch:52, Loss: 0.6092\n",
      "Epoch:53, Loss: 0.4258\n",
      "Epoch:54, Loss: 0.3988\n",
      "Epoch:55, Loss: 0.5315\n",
      "Epoch:56, Loss: 0.6241\n",
      "Epoch:57, Loss: 0.7198\n",
      "Epoch:58, Loss: 0.5740\n",
      "Epoch:59, Loss: 0.4724\n",
      "Epoch:60, Loss: 0.6041\n",
      "Epoch:61, Loss: 0.5963\n",
      "Epoch:62, Loss: 0.6143\n",
      "Epoch:63, Loss: 0.5505\n",
      "Epoch:64, Loss: 0.5518\n",
      "Epoch:65, Loss: 0.5060\n",
      "Epoch:66, Loss: 0.5610\n",
      "Epoch:67, Loss: 0.4962\n",
      "Epoch:68, Loss: 0.5331\n",
      "Epoch:69, Loss: 0.7199\n",
      "Epoch:70, Loss: 0.4029\n",
      "Epoch:71, Loss: 0.4517\n",
      "Epoch:72, Loss: 0.5803\n",
      "Epoch:73, Loss: 0.6195\n",
      "Epoch:74, Loss: 0.6156\n",
      "Epoch:75, Loss: 0.5596\n",
      "Epoch:76, Loss: 0.5115\n",
      "Epoch:77, Loss: 0.6124\n",
      "Epoch:78, Loss: 0.5960\n",
      "Epoch:79, Loss: 0.5472\n",
      "Epoch:80, Loss: 0.5211\n",
      "Epoch:81, Loss: 0.4890\n",
      "Epoch:82, Loss: 0.4706\n",
      "Epoch:83, Loss: 0.6759\n",
      "Epoch:84, Loss: 0.7215\n",
      "Epoch:85, Loss: 0.4363\n",
      "Epoch:86, Loss: 0.5381\n",
      "Epoch:87, Loss: 0.5846\n",
      "Epoch:88, Loss: 0.6326\n",
      "Epoch:89, Loss: 0.5148\n",
      "Epoch:90, Loss: 0.5866\n",
      "Epoch:91, Loss: 0.5987\n",
      "Epoch:92, Loss: 0.6484\n",
      "Epoch:93, Loss: 0.5422\n",
      "Epoch:94, Loss: 0.4841\n",
      "Epoch:95, Loss: 0.5822\n",
      "Epoch:96, Loss: 0.5935\n",
      "Epoch:97, Loss: 0.5289\n",
      "Epoch:98, Loss: 0.6802\n",
      "Epoch:99, Loss: 0.6276\n",
      "Epoch:100, Loss: 0.6010\n",
      "Epoch:101, Loss: 0.4599\n",
      "Epoch:102, Loss: 0.4914\n",
      "Epoch:103, Loss: 0.5080\n",
      "Epoch:104, Loss: 0.4727\n",
      "Epoch:105, Loss: 0.5079\n",
      "Epoch:106, Loss: 0.5358\n",
      "Epoch:107, Loss: 0.5365\n",
      "Epoch:108, Loss: 0.4408\n",
      "Epoch:109, Loss: 0.5052\n",
      "Epoch:110, Loss: 0.7287\n",
      "Epoch:111, Loss: 0.4431\n",
      "Epoch:112, Loss: 0.5765\n",
      "Epoch:113, Loss: 0.5693\n",
      "Epoch:114, Loss: 0.4750\n",
      "Epoch:115, Loss: 0.6301\n",
      "Epoch:116, Loss: 0.5549\n",
      "Epoch:117, Loss: 0.4501\n",
      "Epoch:118, Loss: 0.5177\n",
      "Epoch:119, Loss: 0.4151\n",
      "Epoch:120, Loss: 0.5169\n",
      "Epoch:121, Loss: 0.4882\n",
      "Epoch:122, Loss: 0.4972\n",
      "Epoch:123, Loss: 0.7695\n",
      "Epoch:124, Loss: 0.4914\n",
      "Epoch:125, Loss: 0.5872\n",
      "Epoch:126, Loss: 0.5564\n",
      "Epoch:127, Loss: 0.5502\n",
      "Epoch:128, Loss: 0.4560\n",
      "Epoch:129, Loss: 0.5963\n",
      "Epoch:130, Loss: 0.5141\n",
      "Epoch:131, Loss: 0.5416\n",
      "Epoch:132, Loss: 0.6035\n",
      "Epoch:133, Loss: 0.6600\n",
      "Epoch:134, Loss: 0.5207\n",
      "Epoch:135, Loss: 0.6969\n",
      "Epoch:136, Loss: 0.6746\n",
      "Epoch:137, Loss: 0.5044\n",
      "Epoch:138, Loss: 0.5725\n",
      "Epoch:139, Loss: 0.5040\n",
      "Epoch:140, Loss: 0.3854\n",
      "Epoch:141, Loss: 0.5118\n",
      "Epoch:142, Loss: 0.4702\n",
      "Epoch:143, Loss: 0.5206\n",
      "Epoch:144, Loss: 0.5321\n",
      "Epoch:145, Loss: 0.6357\n",
      "Epoch:146, Loss: 0.5626\n",
      "Epoch:147, Loss: 0.5600\n",
      "Epoch:148, Loss: 0.5468\n",
      "Epoch:149, Loss: 0.5467\n",
      "Epoch:150, Loss: 0.6228\n",
      "Epoch:151, Loss: 0.4805\n",
      "Epoch:152, Loss: 0.5049\n",
      "Epoch:153, Loss: 0.6023\n",
      "Epoch:154, Loss: 0.4453\n",
      "Epoch:155, Loss: 0.4613\n",
      "Epoch:156, Loss: 0.6864\n",
      "Epoch:157, Loss: 0.6507\n",
      "Epoch:158, Loss: 0.5534\n",
      "Epoch:159, Loss: 0.5666\n",
      "Epoch:160, Loss: 0.4562\n",
      "Epoch:161, Loss: 0.5391\n",
      "Epoch:162, Loss: 0.4833\n",
      "Epoch:163, Loss: 0.6110\n",
      "Epoch:164, Loss: 0.6520\n",
      "Epoch:165, Loss: 0.5501\n",
      "Epoch:166, Loss: 0.3862\n",
      "Epoch:167, Loss: 0.5627\n",
      "Epoch:168, Loss: 0.4760\n",
      "Epoch:169, Loss: 0.4808\n",
      "Epoch:170, Loss: 0.6465\n",
      "Epoch:171, Loss: 0.5983\n",
      "Epoch:172, Loss: 0.5854\n",
      "Epoch:173, Loss: 0.6922\n",
      "Epoch:174, Loss: 0.5328\n",
      "Epoch:175, Loss: 0.4192\n",
      "Epoch:176, Loss: 0.5240\n",
      "Epoch:177, Loss: 0.4711\n",
      "Epoch:178, Loss: 0.5929\n",
      "Epoch:179, Loss: 0.4320\n",
      "Epoch:180, Loss: 0.4867\n",
      "Epoch:181, Loss: 0.5557\n",
      "Epoch:182, Loss: 0.4023\n",
      "Epoch:183, Loss: 0.4311\n",
      "Epoch:184, Loss: 0.6760\n",
      "Epoch:185, Loss: 0.6550\n",
      "Epoch:186, Loss: 0.6806\n",
      "Epoch:187, Loss: 0.7702\n",
      "Epoch:188, Loss: 0.5866\n",
      "Epoch:189, Loss: 0.4309\n",
      "Epoch:190, Loss: 0.4395\n",
      "Epoch:191, Loss: 0.5845\n",
      "Epoch:192, Loss: 0.5901\n",
      "Epoch:193, Loss: 0.6226\n",
      "Epoch:194, Loss: 0.4661\n",
      "Epoch:195, Loss: 0.4920\n",
      "Epoch:196, Loss: 0.4473\n",
      "Epoch:197, Loss: 0.6158\n",
      "Epoch:198, Loss: 0.6691\n",
      "Epoch:199, Loss: 0.4175\n",
      "Epoch:200, Loss: 0.4884\n",
      "Epoch:201, Loss: 0.3598\n",
      "Epoch:202, Loss: 0.5600\n",
      "Epoch:203, Loss: 0.5541\n",
      "Epoch:204, Loss: 0.4529\n",
      "Epoch:205, Loss: 0.5459\n",
      "Epoch:206, Loss: 0.6168\n",
      "Epoch:207, Loss: 0.4878\n",
      "Epoch:208, Loss: 0.4282\n",
      "Epoch:209, Loss: 0.5327\n",
      "Epoch:210, Loss: 0.5134\n",
      "Epoch:211, Loss: 0.5459\n",
      "Epoch:212, Loss: 0.4790\n",
      "Epoch:213, Loss: 0.6360\n",
      "Epoch:214, Loss: 0.6103\n",
      "Epoch:215, Loss: 0.5300\n",
      "Epoch:216, Loss: 0.7641\n",
      "Epoch:217, Loss: 0.4749\n",
      "Epoch:218, Loss: 0.5734\n",
      "Epoch:219, Loss: 0.6432\n",
      "Epoch:220, Loss: 0.5239\n",
      "Epoch:221, Loss: 0.7392\n",
      "Epoch:222, Loss: 0.5193\n",
      "Epoch:223, Loss: 0.4969\n",
      "Epoch:224, Loss: 0.4736\n",
      "Epoch:225, Loss: 0.5871\n",
      "Epoch:226, Loss: 0.6347\n",
      "Epoch:227, Loss: 0.7561\n",
      "Epoch:228, Loss: 0.5806\n",
      "Epoch:229, Loss: 0.4246\n",
      "Epoch:230, Loss: 0.6013\n",
      "Epoch:231, Loss: 0.5333\n",
      "Epoch:232, Loss: 0.5317\n",
      "Epoch:233, Loss: 0.6452\n",
      "Epoch:234, Loss: 0.5652\n",
      "Epoch:235, Loss: 0.4142\n",
      "Epoch:236, Loss: 0.5411\n",
      "Epoch:237, Loss: 0.6204\n",
      "Epoch:238, Loss: 0.4631\n",
      "Epoch:239, Loss: 0.6110\n",
      "Epoch:240, Loss: 0.4503\n",
      "Epoch:241, Loss: 0.5575\n",
      "Epoch:242, Loss: 0.5284\n",
      "Epoch:243, Loss: 0.4808\n",
      "Epoch:244, Loss: 0.5489\n",
      "Epoch:245, Loss: 0.5090\n",
      "Epoch:246, Loss: 0.5847\n",
      "Epoch:247, Loss: 0.5070\n",
      "Epoch:248, Loss: 0.6305\n",
      "Epoch:249, Loss: 0.4872\n",
      "Epoch:250, Loss: 0.5525\n",
      "Epoch:251, Loss: 0.5515\n",
      "Epoch:252, Loss: 0.6533\n",
      "Epoch:253, Loss: 0.5759\n",
      "Epoch:254, Loss: 0.8234\n",
      "Epoch:255, Loss: 0.4843\n",
      "Epoch:256, Loss: 0.4674\n",
      "Epoch:257, Loss: 0.5506\n",
      "Epoch:258, Loss: 0.4484\n",
      "Epoch:259, Loss: 0.5619\n",
      "Epoch:260, Loss: 0.5135\n",
      "Epoch:261, Loss: 0.6382\n",
      "Epoch:262, Loss: 0.4937\n",
      "Epoch:263, Loss: 0.4449\n",
      "Epoch:264, Loss: 0.5525\n",
      "Epoch:265, Loss: 0.5683\n",
      "Epoch:266, Loss: 0.6239\n",
      "Epoch:267, Loss: 0.5858\n",
      "Epoch:268, Loss: 0.4721\n",
      "Epoch:269, Loss: 0.4726\n",
      "Epoch:270, Loss: 0.4436\n",
      "Epoch:271, Loss: 0.4989\n",
      "Epoch:272, Loss: 0.4597\n",
      "Epoch:273, Loss: 0.4952\n",
      "Epoch:274, Loss: 0.5321\n",
      "Epoch:275, Loss: 0.3919\n",
      "Epoch:276, Loss: 0.5265\n",
      "Epoch:277, Loss: 0.5365\n",
      "Epoch:278, Loss: 0.6485\n",
      "Epoch:279, Loss: 0.4945\n",
      "Epoch:280, Loss: 0.6293\n",
      "Epoch:281, Loss: 0.5098\n",
      "Epoch:282, Loss: 0.4178\n",
      "Epoch:283, Loss: 0.6568\n",
      "Epoch:284, Loss: 0.6862\n",
      "Epoch:285, Loss: 0.4524\n",
      "Epoch:286, Loss: 0.5016\n",
      "Epoch:287, Loss: 0.6676\n",
      "Epoch:288, Loss: 0.5634\n",
      "Epoch:289, Loss: 0.6991\n",
      "Epoch:290, Loss: 0.5741\n",
      "Epoch:291, Loss: 0.7293\n",
      "Epoch:292, Loss: 0.5288\n",
      "Epoch:293, Loss: 0.5342\n",
      "Epoch:294, Loss: 0.6349\n",
      "Epoch:295, Loss: 0.6092\n",
      "Epoch:296, Loss: 0.6315\n",
      "Epoch:297, Loss: 0.3886\n",
      "Epoch:298, Loss: 0.5641\n",
      "Epoch:299, Loss: 0.5794\n",
      "Epoch:300, Loss: 0.5429\n",
      "Epoch:301, Loss: 0.5752\n",
      "Epoch:302, Loss: 0.4869\n",
      "Epoch:303, Loss: 0.5808\n",
      "Epoch:304, Loss: 0.5262\n",
      "Epoch:305, Loss: 0.6883\n",
      "Epoch:306, Loss: 0.5843\n",
      "Epoch:307, Loss: 0.4703\n",
      "Epoch:308, Loss: 0.5175\n",
      "Epoch:309, Loss: 0.5569\n",
      "Epoch:310, Loss: 0.5281\n",
      "Epoch:311, Loss: 0.7746\n",
      "Epoch:312, Loss: 0.5970\n",
      "Epoch:313, Loss: 0.6763\n",
      "Epoch:314, Loss: 0.5421\n",
      "Epoch:315, Loss: 0.7666\n",
      "Epoch:316, Loss: 0.4781\n",
      "Epoch:317, Loss: 0.5490\n",
      "Epoch:318, Loss: 0.4829\n",
      "Epoch:319, Loss: 0.6976\n",
      "Epoch:320, Loss: 0.5000\n",
      "Epoch:321, Loss: 0.4637\n",
      "Epoch:322, Loss: 0.5463\n",
      "Epoch:323, Loss: 0.5259\n",
      "Epoch:324, Loss: 0.4644\n",
      "Epoch:325, Loss: 0.5800\n",
      "Epoch:326, Loss: 0.4875\n",
      "Epoch:327, Loss: 0.5963\n",
      "Epoch:328, Loss: 0.4759\n",
      "Epoch:329, Loss: 0.5693\n",
      "Epoch:330, Loss: 0.5546\n",
      "Epoch:331, Loss: 0.5755\n",
      "Epoch:332, Loss: 0.6093\n",
      "Epoch:333, Loss: 0.5294\n",
      "Epoch:334, Loss: 0.5449\n",
      "Epoch:335, Loss: 0.5795\n",
      "Epoch:336, Loss: 0.6933\n",
      "Epoch:337, Loss: 0.6168\n",
      "Epoch:338, Loss: 0.4917\n",
      "Epoch:339, Loss: 0.6215\n",
      "Epoch:340, Loss: 0.6478\n",
      "Epoch:341, Loss: 0.5842\n",
      "Epoch:342, Loss: 0.6299\n",
      "Epoch:343, Loss: 0.5315\n",
      "Epoch:344, Loss: 0.5488\n",
      "Epoch:345, Loss: 0.3630\n",
      "Epoch:346, Loss: 0.6767\n",
      "Epoch:347, Loss: 0.5245\n",
      "Epoch:348, Loss: 0.5278\n",
      "Epoch:349, Loss: 0.5192\n",
      "Epoch:350, Loss: 0.6644\n",
      "Epoch:351, Loss: 0.5448\n",
      "Epoch:352, Loss: 0.7364\n",
      "Epoch:353, Loss: 0.6912\n",
      "Epoch:354, Loss: 0.3892\n",
      "Epoch:355, Loss: 0.4843\n",
      "Epoch:356, Loss: 0.4453\n",
      "Epoch:357, Loss: 0.5658\n",
      "Epoch:358, Loss: 0.4748\n",
      "Epoch:359, Loss: 0.5023\n",
      "Epoch:360, Loss: 0.4583\n",
      "Epoch:361, Loss: 0.5977\n",
      "Epoch:362, Loss: 0.6488\n",
      "Epoch:363, Loss: 0.4946\n",
      "Epoch:364, Loss: 0.7448\n",
      "Epoch:365, Loss: 0.6279\n",
      "Epoch:366, Loss: 0.4000\n",
      "Epoch:367, Loss: 0.4678\n",
      "Epoch:368, Loss: 0.6769\n",
      "Epoch:369, Loss: 0.4822\n",
      "Epoch:370, Loss: 0.3838\n",
      "Epoch:371, Loss: 0.6404\n",
      "Epoch:372, Loss: 0.6459\n",
      "Epoch:373, Loss: 0.6088\n",
      "Epoch:374, Loss: 0.5687\n",
      "Epoch:375, Loss: 0.5693\n",
      "Epoch:376, Loss: 0.4177\n",
      "Epoch:377, Loss: 0.5935\n",
      "Epoch:378, Loss: 0.5855\n",
      "Epoch:379, Loss: 0.5058\n",
      "Epoch:380, Loss: 0.4636\n",
      "Epoch:381, Loss: 0.5577\n",
      "Epoch:382, Loss: 0.3853\n",
      "Epoch:383, Loss: 0.5493\n",
      "Epoch:384, Loss: 0.4561\n",
      "Epoch:385, Loss: 0.6483\n",
      "Epoch:386, Loss: 0.5007\n",
      "Epoch:387, Loss: 0.4557\n",
      "Epoch:388, Loss: 0.5482\n",
      "Epoch:389, Loss: 0.6084\n",
      "Epoch:390, Loss: 0.4520\n",
      "Epoch:391, Loss: 0.5760\n",
      "Epoch:392, Loss: 0.6140\n",
      "Epoch:393, Loss: 0.6054\n",
      "Epoch:394, Loss: 0.4754\n",
      "Epoch:395, Loss: 0.5367\n",
      "Epoch:396, Loss: 0.6068\n",
      "Epoch:397, Loss: 0.5859\n",
      "Epoch:398, Loss: 0.6815\n",
      "Epoch:399, Loss: 0.6209\n",
      "Epoch:400, Loss: 0.6886\n",
      "Epoch:401, Loss: 0.5307\n",
      "Epoch:402, Loss: 0.4501\n",
      "Epoch:403, Loss: 0.7516\n",
      "Epoch:404, Loss: 0.7061\n",
      "Epoch:405, Loss: 0.6794\n",
      "Epoch:406, Loss: 0.6177\n",
      "Epoch:407, Loss: 0.6040\n",
      "Epoch:408, Loss: 0.5175\n",
      "Epoch:409, Loss: 0.4909\n",
      "Epoch:410, Loss: 0.6415\n",
      "Epoch:411, Loss: 0.5638\n",
      "Epoch:412, Loss: 0.6041\n",
      "Epoch:413, Loss: 0.6685\n",
      "Epoch:414, Loss: 0.5447\n",
      "Epoch:415, Loss: 0.5336\n",
      "Epoch:416, Loss: 0.5314\n",
      "Epoch:417, Loss: 0.5439\n",
      "Epoch:418, Loss: 0.4978\n",
      "Epoch:419, Loss: 0.4859\n",
      "Epoch:420, Loss: 0.4343\n",
      "Epoch:421, Loss: 0.5019\n",
      "Epoch:422, Loss: 0.4049\n",
      "Epoch:423, Loss: 0.6230\n",
      "Epoch:424, Loss: 0.5970\n",
      "Epoch:425, Loss: 0.6302\n",
      "Epoch:426, Loss: 0.5356\n",
      "Epoch:427, Loss: 0.7285\n",
      "Epoch:428, Loss: 0.5286\n",
      "Epoch:429, Loss: 0.6675\n",
      "Epoch:430, Loss: 0.6364\n",
      "Epoch:431, Loss: 0.5167\n",
      "Epoch:432, Loss: 0.5107\n",
      "Epoch:433, Loss: 0.6248\n",
      "Epoch:434, Loss: 0.5384\n",
      "Epoch:435, Loss: 0.5665\n",
      "Epoch:436, Loss: 0.6405\n",
      "Epoch:437, Loss: 0.5387\n",
      "Epoch:438, Loss: 0.5582\n",
      "Epoch:439, Loss: 0.4708\n",
      "Epoch:440, Loss: 0.5679\n",
      "Epoch:441, Loss: 0.4385\n",
      "Epoch:442, Loss: 0.5328\n",
      "Epoch:443, Loss: 0.6842\n",
      "Epoch:444, Loss: 0.4819\n",
      "Epoch:445, Loss: 0.5923\n",
      "Epoch:446, Loss: 0.6449\n",
      "Epoch:447, Loss: 0.4709\n",
      "Epoch:448, Loss: 0.6297\n",
      "Epoch:449, Loss: 0.5223\n",
      "Epoch:450, Loss: 0.5429\n",
      "Epoch:451, Loss: 0.6455\n",
      "Epoch:452, Loss: 0.5929\n",
      "Epoch:453, Loss: 0.7573\n",
      "Epoch:454, Loss: 0.6485\n",
      "Epoch:455, Loss: 0.7104\n",
      "Epoch:456, Loss: 0.5668\n",
      "Epoch:457, Loss: 0.4812\n",
      "Epoch:458, Loss: 0.5327\n",
      "Epoch:459, Loss: 0.4790\n",
      "Epoch:460, Loss: 0.5045\n",
      "Epoch:461, Loss: 0.6796\n",
      "Epoch:462, Loss: 0.4477\n",
      "Epoch:463, Loss: 0.5089\n",
      "Epoch:464, Loss: 0.4554\n",
      "Epoch:465, Loss: 0.5513\n",
      "Epoch:466, Loss: 0.5297\n",
      "Epoch:467, Loss: 0.5950\n",
      "Epoch:468, Loss: 0.4556\n",
      "Epoch:469, Loss: 0.6596\n",
      "Epoch:470, Loss: 0.6116\n",
      "Epoch:471, Loss: 0.6391\n",
      "Epoch:472, Loss: 0.5249\n",
      "Epoch:473, Loss: 0.5201\n",
      "Epoch:474, Loss: 0.4843\n",
      "Epoch:475, Loss: 0.5551\n",
      "Epoch:476, Loss: 0.6136\n",
      "Epoch:477, Loss: 0.5780\n",
      "Epoch:478, Loss: 0.6495\n",
      "Epoch:479, Loss: 0.5570\n",
      "Epoch:480, Loss: 0.4364\n",
      "Epoch:481, Loss: 0.4955\n",
      "Epoch:482, Loss: 0.5868\n",
      "Epoch:483, Loss: 0.6406\n",
      "Epoch:484, Loss: 0.7280\n",
      "Epoch:485, Loss: 0.5909\n",
      "Epoch:486, Loss: 0.6366\n",
      "Epoch:487, Loss: 0.4087\n",
      "Epoch:488, Loss: 0.5656\n",
      "Epoch:489, Loss: 0.5559\n",
      "Epoch:490, Loss: 0.5028\n",
      "Epoch:491, Loss: 0.7785\n",
      "Epoch:492, Loss: 0.5022\n",
      "Epoch:493, Loss: 0.5462\n",
      "Epoch:494, Loss: 0.4336\n",
      "Epoch:495, Loss: 0.6374\n",
      "Epoch:496, Loss: 0.4726\n",
      "Epoch:497, Loss: 0.3814\n",
      "Epoch:498, Loss: 0.4905\n",
      "Epoch:499, Loss: 0.3944\n",
      "Epoch:500, Loss: 0.3992\n",
      "Epoch:501, Loss: 0.7327\n",
      "Epoch:502, Loss: 0.6151\n",
      "Epoch:503, Loss: 0.5308\n",
      "Epoch:504, Loss: 0.5465\n",
      "Epoch:505, Loss: 0.6216\n",
      "Epoch:506, Loss: 0.5283\n",
      "Epoch:507, Loss: 0.4454\n",
      "Epoch:508, Loss: 0.5229\n",
      "Epoch:509, Loss: 0.7060\n",
      "Epoch:510, Loss: 0.6046\n",
      "Epoch:511, Loss: 0.4163\n",
      "Epoch:512, Loss: 0.4747\n",
      "Epoch:513, Loss: 0.5436\n",
      "Epoch:514, Loss: 0.5126\n",
      "Epoch:515, Loss: 0.5825\n",
      "Epoch:516, Loss: 0.6408\n",
      "Epoch:517, Loss: 0.5902\n",
      "Epoch:518, Loss: 0.6067\n",
      "Epoch:519, Loss: 0.6064\n",
      "Epoch:520, Loss: 0.5226\n",
      "Epoch:521, Loss: 0.6249\n",
      "Epoch:522, Loss: 0.5276\n",
      "Epoch:523, Loss: 0.5803\n",
      "Epoch:524, Loss: 0.4368\n",
      "Epoch:525, Loss: 0.6357\n",
      "Epoch:526, Loss: 0.5978\n",
      "Epoch:527, Loss: 0.4547\n",
      "Epoch:528, Loss: 0.7281\n",
      "Epoch:529, Loss: 0.4552\n",
      "Epoch:530, Loss: 0.4692\n",
      "Epoch:531, Loss: 0.4808\n",
      "Epoch:532, Loss: 0.6099\n",
      "Epoch:533, Loss: 0.5850\n",
      "Epoch:534, Loss: 0.6260\n",
      "Epoch:535, Loss: 0.6549\n",
      "Epoch:536, Loss: 0.5284\n",
      "Epoch:537, Loss: 0.5599\n",
      "Epoch:538, Loss: 0.8064\n",
      "Epoch:539, Loss: 0.4802\n",
      "Epoch:540, Loss: 0.5819\n",
      "Epoch:541, Loss: 0.5616\n",
      "Epoch:542, Loss: 0.5165\n",
      "Epoch:543, Loss: 0.5294\n",
      "Epoch:544, Loss: 0.5670\n",
      "Epoch:545, Loss: 0.5443\n",
      "Epoch:546, Loss: 0.5073\n",
      "Epoch:547, Loss: 0.4263\n",
      "Epoch:548, Loss: 0.5707\n",
      "Epoch:549, Loss: 0.6678\n",
      "Epoch:550, Loss: 0.5521\n",
      "Epoch:551, Loss: 0.5563\n",
      "Epoch:552, Loss: 0.7311\n",
      "Epoch:553, Loss: 0.4810\n",
      "Epoch:554, Loss: 0.5863\n",
      "Epoch:555, Loss: 0.5917\n",
      "Epoch:556, Loss: 0.4164\n",
      "Epoch:557, Loss: 0.4557\n",
      "Epoch:558, Loss: 0.4262\n",
      "Epoch:559, Loss: 0.4782\n",
      "Epoch:560, Loss: 0.5564\n",
      "Epoch:561, Loss: 0.6382\n",
      "Epoch:562, Loss: 0.5104\n",
      "Epoch:563, Loss: 0.5418\n",
      "Epoch:564, Loss: 0.5382\n",
      "Epoch:565, Loss: 0.4308\n",
      "Epoch:566, Loss: 0.6812\n",
      "Epoch:567, Loss: 0.6626\n",
      "Epoch:568, Loss: 0.6312\n",
      "Epoch:569, Loss: 0.8347\n",
      "Epoch:570, Loss: 0.5784\n",
      "Epoch:571, Loss: 0.5830\n",
      "Epoch:572, Loss: 0.6647\n",
      "Epoch:573, Loss: 0.7639\n",
      "Epoch:574, Loss: 0.5911\n",
      "Epoch:575, Loss: 0.5625\n",
      "Epoch:576, Loss: 0.5425\n",
      "Epoch:577, Loss: 0.5873\n",
      "Epoch:578, Loss: 0.5576\n",
      "Epoch:579, Loss: 0.6067\n",
      "Epoch:580, Loss: 0.6584\n",
      "Epoch:581, Loss: 0.5332\n",
      "Epoch:582, Loss: 0.5880\n",
      "Epoch:583, Loss: 0.5112\n",
      "Epoch:584, Loss: 0.5582\n",
      "Epoch:585, Loss: 0.5845\n",
      "Epoch:586, Loss: 0.6962\n",
      "Epoch:587, Loss: 0.4983\n",
      "Epoch:588, Loss: 0.4361\n",
      "Epoch:589, Loss: 0.5486\n",
      "Epoch:590, Loss: 0.5685\n",
      "Epoch:591, Loss: 0.5773\n",
      "Epoch:592, Loss: 0.6232\n",
      "Epoch:593, Loss: 0.5502\n",
      "Epoch:594, Loss: 0.5651\n",
      "Epoch:595, Loss: 0.5255\n",
      "Epoch:596, Loss: 0.6290\n",
      "Epoch:597, Loss: 0.4423\n",
      "Epoch:598, Loss: 0.5622\n",
      "Epoch:599, Loss: 0.5586\n",
      "Epoch:600, Loss: 0.5307\n",
      "Epoch:601, Loss: 0.5666\n",
      "Epoch:602, Loss: 0.5728\n",
      "Epoch:603, Loss: 0.5958\n",
      "Epoch:604, Loss: 0.4416\n",
      "Epoch:605, Loss: 0.5097\n",
      "Epoch:606, Loss: 0.5724\n",
      "Epoch:607, Loss: 0.5965\n",
      "Epoch:608, Loss: 0.5426\n",
      "Epoch:609, Loss: 0.6909\n",
      "Epoch:610, Loss: 0.5638\n",
      "Epoch:611, Loss: 0.6242\n",
      "Epoch:612, Loss: 0.5323\n",
      "Epoch:613, Loss: 0.3841\n",
      "Epoch:614, Loss: 0.7030\n",
      "Epoch:615, Loss: 0.5366\n",
      "Epoch:616, Loss: 0.4530\n",
      "Epoch:617, Loss: 0.5957\n",
      "Epoch:618, Loss: 0.5237\n",
      "Epoch:619, Loss: 0.6002\n",
      "Epoch:620, Loss: 0.6280\n",
      "Epoch:621, Loss: 0.4551\n",
      "Epoch:622, Loss: 0.4881\n",
      "Epoch:623, Loss: 0.4835\n",
      "Epoch:624, Loss: 0.5482\n",
      "Epoch:625, Loss: 0.8087\n",
      "Epoch:626, Loss: 0.5032\n",
      "Epoch:627, Loss: 0.4477\n",
      "Epoch:628, Loss: 0.5929\n",
      "Epoch:629, Loss: 0.6090\n",
      "Epoch:630, Loss: 0.6338\n",
      "Epoch:631, Loss: 0.5222\n",
      "Epoch:632, Loss: 0.5087\n",
      "Epoch:633, Loss: 0.4466\n",
      "Epoch:634, Loss: 0.5913\n",
      "Epoch:635, Loss: 0.6223\n",
      "Epoch:636, Loss: 0.4864\n",
      "Epoch:637, Loss: 0.5555\n",
      "Epoch:638, Loss: 0.6215\n",
      "Epoch:639, Loss: 0.4501\n",
      "Epoch:640, Loss: 0.6007\n",
      "Epoch:641, Loss: 0.4900\n",
      "Epoch:642, Loss: 0.4939\n",
      "Epoch:643, Loss: 0.5695\n",
      "Epoch:644, Loss: 0.4326\n",
      "Epoch:645, Loss: 0.5397\n",
      "Epoch:646, Loss: 0.5356\n",
      "Epoch:647, Loss: 0.6250\n",
      "Epoch:648, Loss: 0.5777\n",
      "Epoch:649, Loss: 0.5922\n",
      "Epoch:650, Loss: 0.6957\n",
      "Epoch:651, Loss: 0.4513\n",
      "Epoch:652, Loss: 0.5811\n",
      "Epoch:653, Loss: 0.6370\n",
      "Epoch:654, Loss: 0.7166\n",
      "Epoch:655, Loss: 0.4614\n",
      "Epoch:656, Loss: 0.6442\n",
      "Epoch:657, Loss: 0.6400\n",
      "Epoch:658, Loss: 0.7585\n",
      "Epoch:659, Loss: 0.5663\n",
      "Epoch:660, Loss: 0.6153\n",
      "Epoch:661, Loss: 0.6234\n",
      "Epoch:662, Loss: 0.5382\n",
      "Epoch:663, Loss: 0.6049\n",
      "Epoch:664, Loss: 0.5746\n",
      "Epoch:665, Loss: 0.4480\n",
      "Epoch:666, Loss: 0.5968\n",
      "Epoch:667, Loss: 0.6296\n",
      "Epoch:668, Loss: 0.7021\n",
      "Epoch:669, Loss: 0.6144\n",
      "Epoch:670, Loss: 0.5517\n",
      "Epoch:671, Loss: 0.6546\n",
      "Epoch:672, Loss: 0.5976\n",
      "Epoch:673, Loss: 0.7242\n",
      "Epoch:674, Loss: 0.5084\n",
      "Epoch:675, Loss: 0.5769\n",
      "Epoch:676, Loss: 0.4860\n",
      "Epoch:677, Loss: 0.5437\n",
      "Epoch:678, Loss: 0.4646\n",
      "Epoch:679, Loss: 0.4801\n",
      "Epoch:680, Loss: 0.7841\n",
      "Epoch:681, Loss: 0.5142\n",
      "Epoch:682, Loss: 0.4557\n",
      "Epoch:683, Loss: 0.5553\n",
      "Epoch:684, Loss: 0.4714\n",
      "Epoch:685, Loss: 0.5424\n",
      "Epoch:686, Loss: 0.4883\n",
      "Epoch:687, Loss: 0.6283\n",
      "Epoch:688, Loss: 0.6380\n",
      "Epoch:689, Loss: 0.6097\n",
      "Epoch:690, Loss: 0.5765\n",
      "Epoch:691, Loss: 0.6012\n",
      "Epoch:692, Loss: 0.5637\n",
      "Epoch:693, Loss: 0.5929\n",
      "Epoch:694, Loss: 0.5957\n",
      "Epoch:695, Loss: 0.5029\n",
      "Epoch:696, Loss: 0.6777\n",
      "Epoch:697, Loss: 0.5372\n",
      "Epoch:698, Loss: 0.5258\n",
      "Epoch:699, Loss: 0.5563\n",
      "Epoch:700, Loss: 0.6225\n",
      "Epoch:701, Loss: 0.5454\n",
      "Epoch:702, Loss: 0.4229\n",
      "Epoch:703, Loss: 0.5632\n",
      "Epoch:704, Loss: 0.5183\n",
      "Epoch:705, Loss: 0.6798\n",
      "Epoch:706, Loss: 0.5470\n",
      "Epoch:707, Loss: 0.3937\n",
      "Epoch:708, Loss: 0.5107\n",
      "Epoch:709, Loss: 0.6061\n",
      "Epoch:710, Loss: 0.5882\n",
      "Epoch:711, Loss: 0.5445\n",
      "Epoch:712, Loss: 0.5143\n",
      "Epoch:713, Loss: 0.6064\n",
      "Epoch:714, Loss: 0.5222\n",
      "Epoch:715, Loss: 0.4617\n",
      "Epoch:716, Loss: 0.4408\n",
      "Epoch:717, Loss: 0.3838\n",
      "Epoch:718, Loss: 0.5484\n",
      "Epoch:719, Loss: 0.6305\n",
      "Epoch:720, Loss: 0.4739\n",
      "Epoch:721, Loss: 0.7125\n",
      "Epoch:722, Loss: 0.6171\n",
      "Epoch:723, Loss: 0.5450\n",
      "Epoch:724, Loss: 0.6253\n",
      "Epoch:725, Loss: 0.7843\n",
      "Epoch:726, Loss: 0.5219\n",
      "Epoch:727, Loss: 0.5937\n",
      "Epoch:728, Loss: 0.5993\n",
      "Epoch:729, Loss: 0.4996\n",
      "Epoch:730, Loss: 0.6891\n",
      "Epoch:731, Loss: 0.4221\n",
      "Epoch:732, Loss: 0.5316\n",
      "Epoch:733, Loss: 0.5993\n",
      "Epoch:734, Loss: 0.4634\n",
      "Epoch:735, Loss: 0.4999\n",
      "Epoch:736, Loss: 0.4910\n",
      "Epoch:737, Loss: 0.6382\n",
      "Epoch:738, Loss: 0.6884\n",
      "Epoch:739, Loss: 0.4309\n",
      "Epoch:740, Loss: 0.5278\n",
      "Epoch:741, Loss: 0.5558\n",
      "Epoch:742, Loss: 0.5435\n",
      "Epoch:743, Loss: 0.5884\n",
      "Epoch:744, Loss: 0.6583\n",
      "Epoch:745, Loss: 0.5658\n",
      "Epoch:746, Loss: 0.3977\n",
      "Epoch:747, Loss: 0.5623\n",
      "Epoch:748, Loss: 0.5766\n",
      "Epoch:749, Loss: 0.6931\n",
      "Epoch:750, Loss: 0.5472\n",
      "Epoch:751, Loss: 0.4975\n",
      "Epoch:752, Loss: 0.4699\n",
      "Epoch:753, Loss: 0.5959\n",
      "Epoch:754, Loss: 0.5448\n",
      "Epoch:755, Loss: 0.5147\n",
      "Epoch:756, Loss: 0.5269\n",
      "Epoch:757, Loss: 0.7698\n",
      "Epoch:758, Loss: 0.6437\n",
      "Epoch:759, Loss: 0.4531\n",
      "Epoch:760, Loss: 0.6680\n",
      "Epoch:761, Loss: 0.5841\n",
      "Epoch:762, Loss: 0.3753\n",
      "Epoch:763, Loss: 0.5402\n",
      "Epoch:764, Loss: 0.4205\n",
      "Epoch:765, Loss: 0.4539\n",
      "Epoch:766, Loss: 0.5631\n",
      "Epoch:767, Loss: 0.5739\n",
      "Epoch:768, Loss: 0.6073\n",
      "Epoch:769, Loss: 0.5864\n",
      "Epoch:770, Loss: 0.6440\n",
      "Epoch:771, Loss: 0.4685\n",
      "Epoch:772, Loss: 0.3819\n",
      "Epoch:773, Loss: 0.5973\n",
      "Epoch:774, Loss: 0.5147\n",
      "Epoch:775, Loss: 0.4331\n",
      "Epoch:776, Loss: 0.7921\n",
      "Epoch:777, Loss: 0.4649\n",
      "Epoch:778, Loss: 0.5733\n",
      "Epoch:779, Loss: 0.5606\n",
      "Epoch:780, Loss: 0.6278\n",
      "Epoch:781, Loss: 0.5703\n",
      "Epoch:782, Loss: 0.5322\n",
      "Epoch:783, Loss: 0.6026\n",
      "Epoch:784, Loss: 0.5472\n",
      "Epoch:785, Loss: 0.6377\n",
      "Epoch:786, Loss: 0.5978\n",
      "Epoch:787, Loss: 0.5336\n",
      "Epoch:788, Loss: 0.5911\n",
      "Epoch:789, Loss: 0.5374\n",
      "Epoch:790, Loss: 0.5572\n",
      "Epoch:791, Loss: 0.4271\n",
      "Epoch:792, Loss: 0.7617\n",
      "Epoch:793, Loss: 0.5340\n",
      "Epoch:794, Loss: 0.5830\n",
      "Epoch:795, Loss: 0.6255\n",
      "Epoch:796, Loss: 0.6347\n",
      "Epoch:797, Loss: 0.6186\n",
      "Epoch:798, Loss: 0.4649\n",
      "Epoch:799, Loss: 0.5351\n",
      "Epoch:800, Loss: 0.4904\n",
      "Epoch:801, Loss: 0.5853\n",
      "Epoch:802, Loss: 0.5394\n",
      "Epoch:803, Loss: 0.4911\n",
      "Epoch:804, Loss: 0.4999\n",
      "Epoch:805, Loss: 0.6521\n",
      "Epoch:806, Loss: 0.6168\n",
      "Epoch:807, Loss: 0.6336\n",
      "Epoch:808, Loss: 0.5273\n",
      "Epoch:809, Loss: 0.5131\n",
      "Epoch:810, Loss: 0.5678\n",
      "Epoch:811, Loss: 0.5693\n",
      "Epoch:812, Loss: 0.5571\n",
      "Epoch:813, Loss: 0.4517\n",
      "Epoch:814, Loss: 0.6140\n",
      "Epoch:815, Loss: 0.5402\n",
      "Epoch:816, Loss: 0.4526\n",
      "Epoch:817, Loss: 0.6161\n",
      "Epoch:818, Loss: 0.5593\n",
      "Epoch:819, Loss: 0.5285\n",
      "Epoch:820, Loss: 0.3993\n",
      "Epoch:821, Loss: 0.6224\n",
      "Epoch:822, Loss: 0.6219\n",
      "Epoch:823, Loss: 0.6330\n",
      "Epoch:824, Loss: 0.5035\n",
      "Epoch:825, Loss: 0.5846\n",
      "Epoch:826, Loss: 0.7215\n",
      "Epoch:827, Loss: 0.6178\n",
      "Epoch:828, Loss: 0.5648\n",
      "Epoch:829, Loss: 0.5971\n",
      "Epoch:830, Loss: 0.4107\n",
      "Epoch:831, Loss: 0.5890\n",
      "Epoch:832, Loss: 0.5434\n",
      "Epoch:833, Loss: 0.7210\n",
      "Epoch:834, Loss: 0.4300\n",
      "Epoch:835, Loss: 0.4402\n",
      "Epoch:836, Loss: 0.5324\n",
      "Epoch:837, Loss: 0.6865\n",
      "Epoch:838, Loss: 0.6349\n",
      "Epoch:839, Loss: 0.6204\n",
      "Epoch:840, Loss: 0.6490\n",
      "Epoch:841, Loss: 0.5509\n",
      "Epoch:842, Loss: 0.6457\n",
      "Epoch:843, Loss: 0.6951\n",
      "Epoch:844, Loss: 0.5807\n",
      "Epoch:845, Loss: 0.5089\n",
      "Epoch:846, Loss: 0.5235\n",
      "Epoch:847, Loss: 0.5479\n",
      "Epoch:848, Loss: 0.5874\n",
      "Epoch:849, Loss: 0.5792\n",
      "Epoch:850, Loss: 0.6475\n",
      "Epoch:851, Loss: 0.5843\n",
      "Epoch:852, Loss: 0.5613\n",
      "Epoch:853, Loss: 0.6009\n",
      "Epoch:854, Loss: 0.6113\n",
      "Epoch:855, Loss: 0.5141\n",
      "Epoch:856, Loss: 0.4501\n",
      "Epoch:857, Loss: 0.6176\n",
      "Epoch:858, Loss: 0.5195\n",
      "Epoch:859, Loss: 0.5562\n",
      "Epoch:860, Loss: 0.5613\n",
      "Epoch:861, Loss: 0.4750\n",
      "Epoch:862, Loss: 0.7481\n",
      "Epoch:863, Loss: 0.6994\n",
      "Epoch:864, Loss: 0.4763\n",
      "Epoch:865, Loss: 0.5138\n",
      "Epoch:866, Loss: 0.5823\n",
      "Epoch:867, Loss: 0.5741\n",
      "Epoch:868, Loss: 0.5882\n",
      "Epoch:869, Loss: 0.5492\n",
      "Epoch:870, Loss: 0.4813\n",
      "Epoch:871, Loss: 0.5414\n",
      "Epoch:872, Loss: 0.6654\n",
      "Epoch:873, Loss: 0.4508\n",
      "Epoch:874, Loss: 0.5192\n",
      "Epoch:875, Loss: 0.4871\n",
      "Epoch:876, Loss: 0.3858\n",
      "Epoch:877, Loss: 0.5806\n",
      "Epoch:878, Loss: 0.6221\n",
      "Epoch:879, Loss: 0.5086\n",
      "Epoch:880, Loss: 0.4654\n",
      "Epoch:881, Loss: 0.5337\n",
      "Epoch:882, Loss: 0.5156\n",
      "Epoch:883, Loss: 0.5935\n",
      "Epoch:884, Loss: 0.5838\n",
      "Epoch:885, Loss: 0.5210\n",
      "Epoch:886, Loss: 0.6870\n",
      "Epoch:887, Loss: 0.5437\n",
      "Epoch:888, Loss: 0.6182\n",
      "Epoch:889, Loss: 0.6291\n",
      "Epoch:890, Loss: 0.4885\n",
      "Epoch:891, Loss: 0.5756\n",
      "Epoch:892, Loss: 0.6654\n",
      "Epoch:893, Loss: 0.5764\n",
      "Epoch:894, Loss: 0.5636\n",
      "Epoch:895, Loss: 0.6473\n",
      "Epoch:896, Loss: 0.5855\n",
      "Epoch:897, Loss: 0.6724\n",
      "Epoch:898, Loss: 0.6952\n",
      "Epoch:899, Loss: 0.5155\n",
      "Epoch:900, Loss: 0.5053\n",
      "Epoch:901, Loss: 0.5581\n",
      "Epoch:902, Loss: 0.6057\n",
      "Epoch:903, Loss: 0.5441\n",
      "Epoch:904, Loss: 0.4774\n",
      "Epoch:905, Loss: 0.4260\n",
      "Epoch:906, Loss: 0.6702\n",
      "Epoch:907, Loss: 0.5338\n",
      "Epoch:908, Loss: 0.5650\n",
      "Epoch:909, Loss: 0.5864\n",
      "Epoch:910, Loss: 0.5286\n",
      "Epoch:911, Loss: 0.7668\n",
      "Epoch:912, Loss: 0.6231\n",
      "Epoch:913, Loss: 0.4546\n",
      "Epoch:914, Loss: 0.5292\n",
      "Epoch:915, Loss: 0.5452\n",
      "Epoch:916, Loss: 0.6029\n",
      "Epoch:917, Loss: 0.4415\n",
      "Epoch:918, Loss: 0.5432\n",
      "Epoch:919, Loss: 0.6696\n",
      "Epoch:920, Loss: 0.4363\n",
      "Epoch:921, Loss: 0.5405\n",
      "Epoch:922, Loss: 0.6821\n",
      "Epoch:923, Loss: 0.5318\n",
      "Epoch:924, Loss: 0.6831\n",
      "Epoch:925, Loss: 0.5248\n",
      "Epoch:926, Loss: 0.5128\n",
      "Epoch:927, Loss: 0.5481\n",
      "Epoch:928, Loss: 0.6184\n",
      "Epoch:929, Loss: 0.5384\n",
      "Epoch:930, Loss: 0.3953\n",
      "Epoch:931, Loss: 0.4948\n",
      "Epoch:932, Loss: 0.4990\n",
      "Epoch:933, Loss: 0.5524\n",
      "Epoch:934, Loss: 0.5375\n",
      "Epoch:935, Loss: 0.5212\n",
      "Epoch:936, Loss: 0.6204\n",
      "Epoch:937, Loss: 0.5813\n",
      "Epoch:938, Loss: 0.4021\n",
      "Epoch:939, Loss: 0.5249\n",
      "Epoch:940, Loss: 0.4409\n",
      "Epoch:941, Loss: 0.6306\n",
      "Epoch:942, Loss: 0.4568\n",
      "Epoch:943, Loss: 0.5767\n",
      "Epoch:944, Loss: 0.4242\n",
      "Epoch:945, Loss: 0.5384\n",
      "Epoch:946, Loss: 0.6136\n",
      "Epoch:947, Loss: 0.5875\n",
      "Epoch:948, Loss: 0.5802\n",
      "Epoch:949, Loss: 0.3667\n",
      "Epoch:950, Loss: 0.6622\n",
      "Epoch:951, Loss: 0.4939\n",
      "Epoch:952, Loss: 0.5115\n",
      "Epoch:953, Loss: 0.6309\n",
      "Epoch:954, Loss: 0.6055\n",
      "Epoch:955, Loss: 0.5260\n",
      "Epoch:956, Loss: 0.5926\n",
      "Epoch:957, Loss: 0.6059\n",
      "Epoch:958, Loss: 0.5767\n",
      "Epoch:959, Loss: 0.5208\n",
      "Epoch:960, Loss: 0.5530\n",
      "Epoch:961, Loss: 0.5529\n",
      "Epoch:962, Loss: 0.6219\n",
      "Epoch:963, Loss: 0.5905\n",
      "Epoch:964, Loss: 0.4211\n",
      "Epoch:965, Loss: 0.5898\n",
      "Epoch:966, Loss: 0.6044\n",
      "Epoch:967, Loss: 0.6498\n",
      "Epoch:968, Loss: 0.5358\n",
      "Epoch:969, Loss: 0.6458\n",
      "Epoch:970, Loss: 0.5641\n",
      "Epoch:971, Loss: 0.4789\n",
      "Epoch:972, Loss: 0.5840\n",
      "Epoch:973, Loss: 0.5127\n",
      "Epoch:974, Loss: 0.6188\n",
      "Epoch:975, Loss: 0.5383\n",
      "Epoch:976, Loss: 0.5580\n",
      "Epoch:977, Loss: 0.6748\n",
      "Epoch:978, Loss: 0.5901\n",
      "Epoch:979, Loss: 0.5791\n",
      "Epoch:980, Loss: 0.4998\n",
      "Epoch:981, Loss: 0.6131\n",
      "Epoch:982, Loss: 0.5553\n",
      "Epoch:983, Loss: 0.4269\n",
      "Epoch:984, Loss: 0.5089\n",
      "Epoch:985, Loss: 0.7166\n",
      "Epoch:986, Loss: 0.4860\n",
      "Epoch:987, Loss: 0.4121\n",
      "Epoch:988, Loss: 0.5650\n",
      "Epoch:989, Loss: 0.6413\n",
      "Epoch:990, Loss: 0.5682\n",
      "Epoch:991, Loss: 0.4716\n",
      "Epoch:992, Loss: 0.5144\n",
      "Epoch:993, Loss: 0.4052\n",
      "Epoch:994, Loss: 0.5612\n",
      "Epoch:995, Loss: 0.5383\n",
      "Epoch:996, Loss: 0.6568\n",
      "Epoch:997, Loss: 0.4819\n",
      "Epoch:998, Loss: 0.6487\n",
      "Epoch:999, Loss: 0.6532\n"
     ]
    }
   ],
   "source": [
    "epochs = 1000\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "    print(f\"Epoch:{epoch}, Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74fd9918",
   "metadata": {},
   "outputs": [],
   "source": [
    "    if (epoch + 1) % 50 == 0:\n",
    "        print(f\"Epoch: [{epoch}/{epochs}], Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3b6d441b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction error on testing set: 26.728571428571435\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "all_predicted_labels = []\n",
    "all_test_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model(inputs) \n",
    "\n",
    "        _, predicted = torch.max(outputs, 1) \n",
    "\n",
    "        all_predicted_labels.extend(predicted.numpy())\n",
    "        all_test_labels.extend(labels.numpy())\n",
    "\n",
    "predicted_labels_np = np.array(all_predicted_labels)\n",
    "test_labels_np = np.array(all_test_labels)\n",
    "\n",
    "# Compute prediction error as a percentage\n",
    "prediction_error_test = np.sum(np.abs(predicted_labels_np - test_labels_np)/len(test_labels_np))*100 # Compute the average absolute error percentage\n",
    "print(\"Prediction error on testing set:\", prediction_error_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6bf740ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 73.27%\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "correct, total = 0, 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (preds == labels).sum().item()\n",
    "\n",
    "print(f\"Accuracy: {100 * correct / total:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c9c85314",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"cardio_ann_model.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cd159ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('../datasets/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "abb497fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_no = test.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2b298c45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        26681\n",
       "1        58585\n",
       "2        54339\n",
       "3        17273\n",
       "4        25420\n",
       "         ...  \n",
       "13995    34457\n",
       "13996    40980\n",
       "13997    83726\n",
       "13998    11086\n",
       "13999    26267\n",
       "Name: id, Length: 14000, dtype: int64"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "26a0fdda",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prep, _ = preprocessing(test, scaler = scaler, fit_scaler=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bec95882",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "      <th>ap_hi</th>\n",
       "      <th>ap_lo</th>\n",
       "      <th>cholesterol</th>\n",
       "      <th>gluc</th>\n",
       "      <th>smoke</th>\n",
       "      <th>alco</th>\n",
       "      <th>active</th>\n",
       "      <th>age_yr</th>\n",
       "      <th>bmi</th>\n",
       "      <th>map</th>\n",
       "      <th>pp</th>\n",
       "      <th>lifestyle</th>\n",
       "      <th>x_syndrome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>-1.135796</td>\n",
       "      <td>-1.023103</td>\n",
       "      <td>-0.055968</td>\n",
       "      <td>-0.060952</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.032755</td>\n",
       "      <td>-0.455301</td>\n",
       "      <td>-0.077394</td>\n",
       "      <td>0.011015</td>\n",
       "      <td>0.593478</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.528034</td>\n",
       "      <td>-1.057850</td>\n",
       "      <td>0.005932</td>\n",
       "      <td>-0.035121</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.655774</td>\n",
       "      <td>-0.733935</td>\n",
       "      <td>-0.029968</td>\n",
       "      <td>0.030994</td>\n",
       "      <td>0.593478</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1.295251</td>\n",
       "      <td>0.957492</td>\n",
       "      <td>-0.055968</td>\n",
       "      <td>-0.086783</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.761997</td>\n",
       "      <td>0.188011</td>\n",
       "      <td>-0.101106</td>\n",
       "      <td>0.030994</td>\n",
       "      <td>0.593478</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>1.538356</td>\n",
       "      <td>-0.849366</td>\n",
       "      <td>-0.055968</td>\n",
       "      <td>-0.035121</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.276888</td>\n",
       "      <td>-1.261851</td>\n",
       "      <td>-0.053681</td>\n",
       "      <td>-0.008965</td>\n",
       "      <td>0.593478</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0.322832</td>\n",
       "      <td>0.471030</td>\n",
       "      <td>-0.055968</td>\n",
       "      <td>-0.086783</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.526302</td>\n",
       "      <td>0.238099</td>\n",
       "      <td>-0.101106</td>\n",
       "      <td>0.030994</td>\n",
       "      <td>0.593478</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13995</th>\n",
       "      <td>1</td>\n",
       "      <td>0.201280</td>\n",
       "      <td>0.957492</td>\n",
       "      <td>0.191632</td>\n",
       "      <td>0.016541</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.659430</td>\n",
       "      <td>0.706747</td>\n",
       "      <td>0.088595</td>\n",
       "      <td>0.110912</td>\n",
       "      <td>0.593478</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13996</th>\n",
       "      <td>2</td>\n",
       "      <td>1.416804</td>\n",
       "      <td>2.138899</td>\n",
       "      <td>0.067832</td>\n",
       "      <td>-0.035121</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.208628</td>\n",
       "      <td>1.024810</td>\n",
       "      <td>-0.006256</td>\n",
       "      <td>0.070953</td>\n",
       "      <td>0.593478</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13997</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.649587</td>\n",
       "      <td>0.887997</td>\n",
       "      <td>0.191632</td>\n",
       "      <td>5.182743</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.976681</td>\n",
       "      <td>1.108446</td>\n",
       "      <td>4.831123</td>\n",
       "      <td>-3.884974</td>\n",
       "      <td>0.593478</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13998</th>\n",
       "      <td>1</td>\n",
       "      <td>0.079728</td>\n",
       "      <td>-0.084926</td>\n",
       "      <td>-0.055968</td>\n",
       "      <td>-0.086783</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.469838</td>\n",
       "      <td>-0.123383</td>\n",
       "      <td>-0.101106</td>\n",
       "      <td>0.030994</td>\n",
       "      <td>0.593478</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13999</th>\n",
       "      <td>2</td>\n",
       "      <td>-0.163377</td>\n",
       "      <td>0.887997</td>\n",
       "      <td>0.067832</td>\n",
       "      <td>-0.035121</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.659430</td>\n",
       "      <td>0.838035</td>\n",
       "      <td>-0.006256</td>\n",
       "      <td>0.070953</td>\n",
       "      <td>0.593478</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14000 rows Ã— 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       gender    height    weight     ap_hi     ap_lo  cholesterol  gluc  \\\n",
       "0           1 -1.135796 -1.023103 -0.055968 -0.060952            1     1   \n",
       "1           1 -0.528034 -1.057850  0.005932 -0.035121            1     1   \n",
       "2           2  1.295251  0.957492 -0.055968 -0.086783            2     1   \n",
       "3           2  1.538356 -0.849366 -0.055968 -0.035121            1     1   \n",
       "4           1  0.322832  0.471030 -0.055968 -0.086783            1     1   \n",
       "...       ...       ...       ...       ...       ...          ...   ...   \n",
       "13995       1  0.201280  0.957492  0.191632  0.016541            1     3   \n",
       "13996       2  1.416804  2.138899  0.067832 -0.035121            1     1   \n",
       "13997       1 -0.649587  0.887997  0.191632  5.182743            1     1   \n",
       "13998       1  0.079728 -0.084926 -0.055968 -0.086783            1     1   \n",
       "13999       2 -0.163377  0.887997  0.067832 -0.035121            1     1   \n",
       "\n",
       "       smoke  alco  active    age_yr       bmi       map        pp  lifestyle  \\\n",
       "0          0     0       1 -0.032755 -0.455301 -0.077394  0.011015   0.593478   \n",
       "1          0     0       1  0.655774 -0.733935 -0.029968  0.030994   0.593478   \n",
       "2          0     0       1 -1.761997  0.188011 -0.101106  0.030994   0.593478   \n",
       "3          0     0       1 -0.276888 -1.261851 -0.053681 -0.008965   0.593478   \n",
       "4          0     0       1 -0.526302  0.238099 -0.101106  0.030994   0.593478   \n",
       "...      ...   ...     ...       ...       ...       ...       ...        ...   \n",
       "13995      0     0       1  0.659430  0.706747  0.088595  0.110912   0.593478   \n",
       "13996      0     0       1  1.208628  1.024810 -0.006256  0.070953   0.593478   \n",
       "13997      0     0       1  0.976681  1.108446  4.831123 -3.884974   0.593478   \n",
       "13998      0     0       1 -0.469838 -0.123383 -0.101106  0.030994   0.593478   \n",
       "13999      0     0       1  0.659430  0.838035 -0.006256  0.070953   0.593478   \n",
       "\n",
       "       x_syndrome  \n",
       "0               0  \n",
       "1               0  \n",
       "2               0  \n",
       "3               0  \n",
       "4               0  \n",
       "...           ...  \n",
       "13995           0  \n",
       "13996           0  \n",
       "13997           0  \n",
       "13998           0  \n",
       "13999           0  \n",
       "\n",
       "[14000 rows x 16 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207521e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2d34190d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Predictions saved to 'submission.csv'\n"
     ]
    }
   ],
   "source": [
    "class CVSDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = torch.tensor(data.values, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]\n",
    "\n",
    "\n",
    "model.eval()\n",
    "\n",
    "test_data = CVSDataset(test_prep)\n",
    "test_loader = DataLoader(test_data, batch_size=32, shuffle=False)\n",
    "\n",
    "# ===============================\n",
    "# 4. Predict on Test Data\n",
    "# ===============================\n",
    "all_preds = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "\n",
    "# ===============================\n",
    "# 5. Save Predictions to CSV\n",
    "# ===============================\n",
    "submission = pd.DataFrame({\n",
    "    \"Id\": id_no,  # or your actual test IDs if available\n",
    "    \"Predicted\": all_preds\n",
    "})\n",
    "\n",
    "submission.to_csv(\"submission.csv\", index=False)\n",
    "print(\"âœ… Predictions saved to 'submission.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a142ad44",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bt_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
